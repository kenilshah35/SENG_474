{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Analysis\n",
    "\n",
    "In this lab, we are going to analyze PCA and ICA.\n",
    "\n",
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data is in a four dimensional space, meaning the number of features is 4. The idea is to project the data into a lower dimensional space. This can be useful for various reasons, including computational efficiency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the dataset by plotting feature 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel('Feature 0', fontsize=15)\n",
    "ax.set_ylabel('Feature 1', fontsize=15)\n",
    "targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color, label in zip(targets, colors, [0, 1, 2]):\n",
    "    ax.scatter(X[y == label][:, 0]\n",
    "               , X[y == label][:, 1]\n",
    "               , c=color\n",
    "               , s=50)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement PCA. \n",
    "\n",
    "1. We first will centralize the data. This is basically done by subtracting off the mean for each column.\n",
    "2. Then we want to obtain the principal directions (which, in particular, form an orthogonal basis (unit norm vectors that are orthogonal to each other)).For this, we calculate Eigenvectors and Eigenvalues. One way to do so is to use covariance matrix or perform Singular Value Decomposition (SVD). SVD is basically a way to represent a matrix ($M$) by product of 3 matrices ($U \\times \\Sigma \\times V^T$). We can extract what we want from this factorization, by looking at $U$, in which, columns are eigenvectors of $MM^T$. We also will find Eigenvectors and Eigenvalues using covariance matrix. (refer to the reference in course website for more details)\n",
    "3. Select the eigenvectors we want to use to transform the data\n",
    "4. Construct transformed data from seleceted eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1, center the data\n",
    "mean_vec = np.mean(X, axis=0)\n",
    "X_std = X - mean_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2, calculating covariance matrix\n",
    "cov_mat = (X_std).T.dot((X_std)) / (X_std.shape[0]-1)\n",
    "print(cov_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the eigenvalues are decreasing and correspond to the variances of the new features (the principal components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the eigen decomposition\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "print('Eigenvectors', eig_vecs)\n",
    "print('Eigenvalues', eig_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2 (SVD Approach) we can use SVD function as well\n",
    "U, S, VT = np.linalg.svd(X_std.T)\n",
    "print(U)\n",
    "# print(S)\n",
    "# print(VT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform the data into a new $k$ dimensional subspace, we need to select $k$ eigenvectors. Recall that we have 4 principle components for Iris (data is 4 dimensional). We can look at explained variance to figure out which principle components to choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3, selecting the eigenvectors\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that first and second components account for almost 98% of the variance, so we can project the data into a 2-dimensional space using these two without losing much information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first two eigenvectors \n",
    "eig_vecs[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4, transformed data\n",
    "X_transformed = X_std.dot(eig_vecs[:, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel('Principal Component 1', fontsize=15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize=15)\n",
    "ax.set_title('2 component PCA', fontsize=20)\n",
    "targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color, label in zip(targets, colors, [0, 1, 2]):\n",
    "    ax.scatter(X_transformed[y == label][:, 0]\n",
    "               , X_transformed[y == label][:, 1]\n",
    "               , c=color\n",
    "               , s=50)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the 3rd and 4th components (the less representative ones) to transform the data and see how poor they perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4, transformed data\n",
    "X_transformed = X_std.dot(eig_vecs[:, 2:])\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel('Principal Component 3', fontsize=15)\n",
    "ax.set_ylabel('Principal Component 4', fontsize=15)\n",
    "ax.set_title('2 component PCA', fontsize=20)\n",
    "targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color, label in zip(targets, colors, [0, 1, 2]):\n",
    "    ax.scatter(X_transformed[y == label][:, 0]\n",
    "               , X_transformed[y == label][:, 1]\n",
    "               , c=color\n",
    "               , s=50)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's quickly use scikit's PCA function to redo everything we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "# The next three lines do the job (you can summarize it in 1 line! > decomposition.PCA(n_components=2).fit(X).transform(X))\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_transformed = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_transformed.shape)\n",
    "print(X[5:8])\n",
    "print(X_transformed[5:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax \n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like this now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color, label in zip(targets, colors, [0, 1, 2]):\n",
    "    ax[0].scatter(X[y == label][:, 0]\n",
    "               , X[y == label][:, 1]\n",
    "               , c=color\n",
    "               , s=50,alpha=0.4)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_[0:2], pca.mean_[0:2] + v[0:2],ax[0])\n",
    "ax[0].legend(targets)\n",
    "ax[0].grid()\n",
    "\n",
    "### Now lets change the axis (Plot the principal components)\n",
    "for target, color, label in zip(targets, colors, [0,1,2]):\n",
    "    ax[1].scatter(X_transformed[y==label][:, 0]\n",
    "               , X_transformed[y==label][:, 1]\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax[1].legend(targets)\n",
    "ax[1].grid()\n",
    "\n",
    "\n",
    "# fig.savefig('figures/05.09-PCA-rotation.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(data):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
    "                             subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(data[i].reshape(8, 8),\n",
    "                  cmap='binary', interpolation='nearest',\n",
    "                  clim=(0, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "plot_digits(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "noisy = np.random.normal(digits.data, 4)\n",
    "plot_digits(noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(0.7).fit(noisy)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.transform(noisy)\n",
    "filtered = pca.inverse_transform(components)\n",
    "plot_digits(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA as Eigenface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(faces.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = mpimg.imread()\n",
    "imgplot = plt.imshow(faces.images[2])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=190)\n",
    "## YOu can use RandomizedPCA here for speeding up the process\n",
    "# from sklearn.decomposition import RandomizedPCA\n",
    "# pca = RandomizedPCA(150)\n",
    "pca.fit(faces.data)\n",
    "print(faces.data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize the first eigenvectors (Principal Components)\n",
    "They are creepy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 8, figsize=(9, 4),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the number of required components\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's reconstruct the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "components = pca.transform(faces.data)\n",
    "projected = pca.inverse_transform(components)\n",
    "print(components.shape)\n",
    "print(faces.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),\n",
    "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n",
    "    ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')\n",
    "    \n",
    "ax[0, 0].set_ylabel('full-dim\\ninput')\n",
    "ax[1, 0].set_ylabel('190-dim\\nreconstruction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),\n",
    "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n",
    "    ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')\n",
    "    \n",
    "ax[0, 0].set_ylabel('full-dim\\ninput')\n",
    "ax[1, 0].set_ylabel('750-dim\\nreconstruction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Component Analysis\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Independent Component Analysis is an algorithm to obtain a linear combination of the original data source. ICA attempts to decompose the original data into independent subsets. Lets say we know that the observed data is coming from the \"linear\" combination of independent sources ($X=As$)\n",
    "\n",
    "* $X$ is the observed data\n",
    "* s is the source that we aim at finding it. \n",
    "* A is the mixing matrix\n",
    "\n",
    "\n",
    "We denote $W$ as the inverse of mixing matrix $A$. We can get the job done if we \"approximate\" $W$. Because by multiplying $W$ to both sides we can get the $s$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating 3 source signals\n",
    "time=np.linspace(0,8,200)\n",
    "\n",
    "#signal1 sinusoidal signal\n",
    "#signal2 square signal\n",
    "#signal3 saw tooth signal\n",
    "signal1=np.sin(2*time)\n",
    "signal2=np.sign(np.sin(3*time))\n",
    "signal3=scipy.signal.sawtooth(2*np.pi*time)\n",
    "#create matrix\n",
    "signals=np.c_[signal1,signal2,signal3]\n",
    "\n",
    "# signals+=0.2*np.random.normal(size=signals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mixing them together \n",
    "A=np.array([[1,1,1],[0.5,2,1.0],[1.5,1.0,2.0]])\n",
    "X=signals@A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets try PCA\n",
    "from sklearn.decomposition import  PCA\n",
    "pca = PCA(n_components=3)\n",
    "H = pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn fast ica\n",
    "from sklearn.decomposition import FastICA\n",
    "clf=FastICA(w_init=np.eye(X.shape[1]))\n",
    "signals_skl=clf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=['red','steelblue','orange']\n",
    "ax=plt.figure(figsize=(6,3)).add_subplot(111)\n",
    "for ind,var in enumerate(X.T):\n",
    "    plt.plot(var,label=f'Signal {ind}',c=colors[ind])\n",
    "plt.title('Observation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(16, 6))\n",
    "# ax=plt.figure(figsize=(6,3)).add_subplot(111)\n",
    "for ind,var in enumerate(signals.T):\n",
    "    ax[0].plot(var,label=f'Signal {ind}',c=colors[ind])\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Sources')\n",
    "for ind,var in enumerate(signals_skl.T):\n",
    "    ax[1].plot(var,label=f'Signal {ind}',c=colors[ind])\n",
    "ax[1].set_xlabel('Estimated_ICA')\n",
    "ax[1].legend()\n",
    "for ind,var in enumerate(H.T):\n",
    "    ax[2].plot(var,label=f'Signal {ind}',c=colors[ind])\n",
    "ax[2].set_xlabel('Estimated_PCA')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICA in de-mixing Images:\n",
    "\n",
    "https://www.quora.com/What-is-the-difference-between-PCA-and-ICA\n",
    "https://github.com/awcasella/ICA-on-Mixed-Images/blob/master/ICA%20applied%20in%20images.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conclusion\n",
    "* Both PCA and ICA are unsupervised learning methods since they do not make use of any labels in the computation.\n",
    "* Both methods find a new set of basis vectors for the data.\n",
    "* PCA is more applicable in noise and dimension reductions. \n",
    "* ICA is more useful in decompsing input data (multiverse) into independent components (universes)\n",
    "* PCA maximizes the variance of the projected data along orthogonal directions.\n",
    "* ICA correctly finds the two vectors onto which the projections are independent.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
